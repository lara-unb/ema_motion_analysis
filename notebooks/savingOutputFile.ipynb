{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING OUTPUT FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formato comprimido - Utilizando o formato pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(data_dic, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data_dic, f)\n",
    "\n",
    "        # for key in data_dic.keys():\n",
    "        #     pickle.dump(key, f)\n",
    "        #     pickle.dump(data_dic[key], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = \"hhhh\"\n",
    "v_info = \"skjdfld\"\n",
    "kp_vec = [1, 3, 3]\n",
    "kp_w_scores_vec = [1, 2, 3, 4, 5]\n",
    "video_time = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'fs_info': {'fs':fs},\n",
    "    'v_info': v_info,\n",
    "    'kp': kp_vec,\n",
    "    'kp_w_scores_vec': kp_w_scores_vec,\n",
    "    'video_time': video_time\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file(data, 'aquivo')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('aquivo','rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fs_info': {'fs': 'hhhh'}, 'v_info': 'skjdfld', 'kp': [1, 3, 3], 'kp_w_scores_vec': [1, 2, 3, 4, 5], 'video_time': 10}\n"
     ]
    }
   ],
   "source": [
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padrão de arquivo não comprimido - Melhor para processamentos em tempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../utils/\")\n",
    "import drawing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='../scripts/lite-model_movenet_singlepose_lightning_3.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções Necessárias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joints Dictionary\n",
    "KEYPOINT_DICT = {\n",
    "    'nose':0,\n",
    "    'left_eye':1,\n",
    "    'right_eye':2,\n",
    "    'left_ear':3,\n",
    "    'right_ear':4,\n",
    "    'left_shoulder':5,\n",
    "    'right_shoulder':6,\n",
    "    'left_elbow':7,\n",
    "    'right_elbow':8,\n",
    "    'left_wrist':9,\n",
    "    'right_wrist':10,\n",
    "    'left_hip':11,\n",
    "    'right_hip':12,\n",
    "    'left_knee':13,\n",
    "    'right_knee':14,\n",
    "    'left_ankle':15,\n",
    "    'right_ankle':16\n",
    "} \n",
    "\n",
    "# Joint parings \n",
    "EDGES = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the parings of the desired joints\n",
    "def getPairings(desired_kp, kp_dict, kp_pairings, neural_network):\n",
    "    new_kp_dict = {}\n",
    "    new_kp_pairings = {}\n",
    "\n",
    "    for kp_name in desired_kp:\n",
    "        new_kp_dict[kp_name] = kp_dict[kp_name]\n",
    "\n",
    "    for pos, color in kp_pairings.items():\n",
    "        p1, p2 = pos\n",
    "        if (p1 in list(new_kp_dict.values())) and (p2 in list(new_kp_dict.values())):\n",
    "            p1_tf = list(new_kp_dict.values()).index(p1)\n",
    "            p2_tf = list(new_kp_dict.values()).index(p2)\n",
    "            new_kp_pairings[(p1_tf, p2_tf)] = color\n",
    "\n",
    "    return new_kp_pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select only the desired joints\n",
    "def selectJoints(image, keypoints, desired_keypoints, kp_dict, neural_network):\n",
    "    image_height, image_width, _ = image.shape\n",
    "    print(image.shape)\n",
    "    print(keypoints)\n",
    "\n",
    "    selected_joints = np.zeros([len(desired_keypoints), 2])\n",
    "    selected_joints[:] = np.NaN\n",
    "\n",
    "    if(keypoints != None):\n",
    "        for i in range(len(desired_keypoints)):\n",
    "            joint = desired_keypoints[i]\n",
    "            \n",
    "            idx = list(kp_dict.keys()).index(joint)\n",
    "            selected_joints[i, :] = keypoints[idx, :]\n",
    " \n",
    "    return selected_joints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the keypoints above the trashhold and change vector to numpy array\n",
    "def transformDATA(kp_w_scores_vec, confidence_threshold, frame_width, frame_height):\n",
    "    keypoints_vec = np.zeros([len(list(KEYPOINT_DICT.values())), 2])\n",
    "    \n",
    "    y, x, c = frame_width, frame_height, 3\n",
    "    shaped = np.squeeze(np.multiply(kp_w_scores_vec, [y,x,1]))\n",
    "    \n",
    "    j = 0\n",
    "    for i in range(len(shaped)):\n",
    "        if i in list(KEYPOINT_DICT.values()):\n",
    "            ky, kx, kp_conf = shaped[i]\n",
    "            if kp_conf > confidence_threshold:\n",
    "                keypoints_vec[j, 0] = kx\n",
    "                keypoints_vec[j, 1] = ky\n",
    "            else:\n",
    "                keypoints_vec[j, 0] = np.nan\n",
    "                keypoints_vec[j, 1] = np.nan\n",
    "            j+=1\n",
    "    return keypoints_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAbsolutePath():\n",
    "    absolute_main_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    return absolute_main_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções para a geração do arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setMetadata(video_name, mapping, pairs, video_path, summary=\"None\"):\n",
    "    if video_name == 0:\n",
    "        video_path = 0\n",
    "        \n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        length = 0\n",
    "        fps = 0\n",
    "    \n",
    "    else:\n",
    "        current_path = os.getcwd()\n",
    "        video_path = current_path + video_path\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        print(\"VIDEO PATH: \", video_path)\n",
    "        print(\"CAP: \", cap)\n",
    "\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "\n",
    "    _, image = cap.read()\n",
    "    frame_width = image.shape[0]\n",
    "    frame_height = image.shape[1]\n",
    "    \n",
    "    cap.set(2, 0.0)\n",
    "\n",
    "    file_metadata = {\n",
    "        'video_name': video_name,\n",
    "        'n_frames': length,\n",
    "        'n_points': len(mapping),\n",
    "        'frame_width': frame_width,\n",
    "        'frame_height': frame_height,\n",
    "        'fps': fps,\n",
    "        'keypoints_names': mapping,\n",
    "        'keypoints_pairs': pairs,\n",
    "        'summary': summary\n",
    "    }\n",
    "    \n",
    "    return file_metadata, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToDATA(file_path, data, write_mode='w'):\n",
    "    with open(file_path, write_mode) as f:\n",
    "        f.write(json.dumps(data))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the whole data\n",
    "\n",
    "def saveDATAtoFile(file_path, data):\n",
    "    writeToDATA(file_path, data[\"metadata\"], write_mode='w+')\n",
    "    for i in range(len(data[\"keypoints\"])):\n",
    "        selected_joints = data[\"keypoints\"][i]\n",
    "        file_data = {\n",
    "            'keypoints': selected_joints.tolist()\n",
    "        }\n",
    "        writeToDATA(file_path, file_data, write_mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/../examples/frontal-rafa.mp4\"\n",
    "video_out_path = \"\"\n",
    "profile = ['right_hip', 'left_hip', 'right_knee', 'left_knee','right_ankle', 'left_ankle']\n",
    "file_path = \"out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIDEO PATH:  /Users/admin/Documents/GitHub/ema_motion_analysis/notebooks/../examples/frontal-rafa.mp4\n",
      "CAP:  <VideoCapture 0x152d72f50>\n",
      "(576, 576, 3)\n",
      "[[         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [         nan          nan]\n",
      " [186.90150833 462.77895355]\n",
      " [183.1100235  460.95913696]]\n",
      "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n",
      "END LOOP\n"
     ]
    }
   ],
   "source": [
    "# For the real time loop (example)\n",
    "\n",
    "file_metadata, cap = setMetadata(\"RAFA FRONTAL\", KEYPOINT_DICT, profile, video_path)\n",
    "\n",
    "writeToDATA(file_path, file_metadata, write_mode='w+')\n",
    "try:\n",
    "    while(True):\n",
    "\n",
    "\n",
    "        #...\n",
    "        current_path = os.getcwd()\n",
    "        video_path = current_path + video_path\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "\n",
    "        has_frame, image = cap.read()\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_width = image.shape[0]\n",
    "        frame_height = image.shape[1]\n",
    "        cap.release() \n",
    " \n",
    "        # Reshape image\n",
    "        img = frame.copy()\n",
    "        img = tf.image.resize_with_pad(np.expand_dims(img, axis=0), 192,192)\n",
    "        input_image = tf.cast(img, dtype=tf.float32)\n",
    "        \n",
    "        # Setup input and output \n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        # Make predictions \n",
    "        interpreter.set_tensor(input_details[0]['index'], np.array(input_image))\n",
    "        interpreter.invoke() \n",
    "        keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Key points\n",
    "        keypoints = transformDATA(keypoints_with_scores, 0.3, frame_width, frame_height)\n",
    "\n",
    "        pose_selected = profile\n",
    "\n",
    "        keypoint_pairings = getPairings(pose_selected, KEYPOINT_DICT, EDGES, \"movenet\")\n",
    "        selected_joints = selectJoints(frame, keypoints, pose_selected, KEYPOINT_DICT, 'movenet')\n",
    "        \n",
    "        # Draw the joints and pairings\n",
    "        drawing.draw_connections(frame, selected_joints, keypoint_pairings)\n",
    "        drawing.draw_keypoints(frame, selected_joints)\n",
    "        \n",
    "        cv2.imshow('MoveNet Lightning', frame)\n",
    "\n",
    "        # ESC para sair\n",
    "        k = cv2.waitKey(25) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "        #...\n",
    "        \n",
    "        file_data = {\n",
    "            'keypoints': selected_joints.tolist()\n",
    "        }\n",
    "        print(\"FILE DATA: \" , file_data)\n",
    "    writeToDATA(file_path, file_data, write_mode='a')\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"END LOOP\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "844af5a12dd537c625067274204f56ea8a690c416d33df5d10c956c60f179d54"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
